{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "import operator\n",
    "from collections import Counter\n",
    "\n",
    "# 定义节点类\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self,  label=None, feature_name=None, feature=None,value=None,split_point=None,data_index=None):\n",
    "        self.label = label\n",
    "        self.feature_name = feature_name\n",
    "        self.feature = feature\n",
    "        self.tree = {}\n",
    "        self.value=value\n",
    "        self.split_point=split_point\n",
    "        self.data_index=data_index\n",
    "    '''\n",
    "    def feature_name(self):\n",
    "        return self.feature_name_list(self.feature)\n",
    "    '''\n",
    "    def display(self,feature_name_list=None):\n",
    "        '''\n",
    "        将树打印出来'''\n",
    "        if feature_name_list:featurename=feature_name_list[self.feature] if self.feature!=None else None\n",
    "        else:featurename=self.feature\n",
    "        res={'label':self.label,'feature':featurename,'tree':{}}\n",
    "\n",
    "        if self.value:res['value']=self.value\n",
    "        if self.split_point:res['split_point']=self.split_point\n",
    "        for next_node in self.tree:\n",
    "            res['tree'][next_node]=self.tree[next_node].display(feature_name_list)\n",
    "        return res\n",
    "    def print_leaf_node(self):\n",
    "        res=[]\n",
    "        def dfs(node):\n",
    "            if node.tree=={}:\n",
    "                res.append((node.label,node.data_index))\n",
    "                return\n",
    "            for next_node in node.tree:\n",
    "                dfs(node.tree[next_node])\n",
    "            return\n",
    "        dfs(self)\n",
    "        return res\n",
    "\n",
    "class BaseDecisionTree:\n",
    "    def __init__(self,epsilon=1e-3,min_samples_leaf=1,max_depth=float('inf'),is_gradient=False,K=None):\n",
    "        self.root=None\n",
    "        self.epsilon=epsilon  # 信息增益/信息增益比/Gini小于该阈值时，算法停止\n",
    "        self.min_samples_leaf=min_samples_leaf  #叶子节点拥有的样本最小个数，当节点样本个数小于该阈值时算法停止\n",
    "        self.max_depth = max_depth  #树的最大深度\n",
    "        self.is_gradient=is_gradient   #是否用于GBDT\n",
    "        self.K=K  #用于GBDT分类时的种类\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 criterion,\n",
    "                 splitter,\n",
    "                 max_depth,\n",
    "                 min_samples_split,\n",
    "                 min_samples_leaf,\n",
    "                 min_weight_fraction_leaf,\n",
    "                 max_features,\n",
    "                 max_leaf_nodes,\n",
    "                 random_state,\n",
    "                 min_impurity_decrease,\n",
    "                 min_impurity_split,\n",
    "                 class_weight=None,\n",
    "                 presort=False):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.class_weight = class_weight\n",
    "        self.presort = presort\n",
    "'''\n",
    "    @staticmethod\n",
    "    def entropy(data):\n",
    "        '''\n",
    "        输入数据data,输出其经验熵'''\n",
    "        n=len(data)   #数据个数\n",
    "        label_dict={}\n",
    "        for i in range(n):\n",
    "            label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "        k=len(label_dict)  #类别个数\n",
    "        ent=0\n",
    "        for n_k in label_dict.values():\n",
    "            ent+= n_k/n * math.log(n_k/n,2)\n",
    "        return -ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def conditional_entropy(data,a):\n",
    "        '''\n",
    "        输入数据data和用来分类的特征a(即数据的第a列),输出条件熵'''\n",
    "        n=len(data)   #数据个数\n",
    "        con_ent=0\n",
    "        new_data=BaseDecisionTree.data_divide(data,a)\n",
    "        for curr_data in new_data:\n",
    "            con_ent+= len(curr_data)/n * BaseDecisionTree.entropy(curr_data)        \n",
    "        return con_ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini(data,a=None,value=None):\n",
    "        n=len(data)\n",
    "        if n==0:return 0\n",
    "        if not a:\n",
    "            label_dict={} \n",
    "            for i in range(n):\n",
    "                label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "            return 1-sum((x/n)**2 for x in label_dict.values())\n",
    "        else:\n",
    "            new_data=BaseDecisionTree.data_divide(data,a,value=value)\n",
    "            return len(new_data[0])/n*BaseDecisionTree.gini(new_data[0]) + len(new_data[1])/n*BaseDecisionTree.gini(new_data[1])\n",
    "            \n",
    "    @staticmethod\n",
    "    def data_divide(data,a,data_index=None,value=None):\n",
    "        '''\n",
    "        根据第a列特征将数据划分\n",
    "        如果输入特征a的某个value，将数据集按a=value和a≠value划分成两个\n",
    "        如果没有输入某个value，将数据集按a所有特征划分'''\n",
    "\n",
    "        if not value:\n",
    "            new_data={}\n",
    "            i=0\n",
    "            for curr_data in data:\n",
    "                if data_index==None:next_data=curr_data\n",
    "                else:next_data=(data_index[i],curr_data)\n",
    "                new_data[curr_data[a]]=new_data.get(curr_data[a],[])\n",
    "                new_data[curr_data[a]].append(next_data)\n",
    "                i+=1\n",
    "            return list(new_data.values())\n",
    "        elif type(value)==str:\n",
    "            new_data=[[],[]]\n",
    "            i=0\n",
    "            for curr_data in data:\n",
    "                if data_index==None:next_data=curr_data\n",
    "                else:next_data=(data_index[i],curr_data)\n",
    "                if curr_data[a]==value:\n",
    "                    new_data[0].append(next_data)\n",
    "                else:\n",
    "                    new_data[1].append(next_data)\n",
    "                i+=1\n",
    "            return new_data\n",
    "            \n",
    "        else:\n",
    "            new_data=[[],[]]\n",
    "            i=0\n",
    "            for curr_data in data:\n",
    "                if data_index==None:next_data=curr_data\n",
    "                else:next_data=(data_index[i],curr_data)\n",
    "                if curr_data[a]<value:\n",
    "                    new_data[0].append(next_data)\n",
    "                else:\n",
    "                    new_data[1].append(next_data)\n",
    "                i+=1\n",
    "            return new_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def most_class(data):\n",
    "        '''\n",
    "        返回数据集中实例数最多的类'''\n",
    "        n=len(data)   #数据个数\n",
    "        label_dict={}\n",
    "        for i in range(n):\n",
    "            label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "        m=0\n",
    "        for key in label_dict.keys():\n",
    "            if label_dict[key]>m:\n",
    "                m=label_dict[key]\n",
    "                res=key\n",
    "        return res\n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                curr_node=curr_node.tree[curr_data[curr_node.feature]]\n",
    "            pre.append(curr_node.label)\n",
    "        return pre\n",
    "    \n",
    "class ID3(BaseDecisionTree):\n",
    "    '''\n",
    "    ID3算法\n",
    "    '''\n",
    "\n",
    "    def fit(self,data):\n",
    "        def dfs(new_data,feature_list,depth,data_index):  #递归创建树\n",
    "            if len(new_data)<self.min_samples_leaf or depth>=self.max_depth:  #当前节点样本个数小于阈值，停止\n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                new_node.data_index=data_index\n",
    "                return new_node\n",
    "\n",
    "            best_feature_index,information_gain=self.chooseBestFeature(new_data)  #选取最优的特征\n",
    "            best_feature=feature_list[best_feature_index]\n",
    "\n",
    "            if information_gain<self.epsilon:   #当信息增益小于阈值epsilon，停止\n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                new_node.data_index=data_index\n",
    "                return new_node\n",
    "            \n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.label=ID3.most_class(new_data)\n",
    "            new_node.data_index=data_index\n",
    "            \n",
    "            next_data_with_index_list=ID3.data_divide(new_data,best_feature,data_index=data_index)  #用最优的特征划分当前数据集\n",
    "            \n",
    "            for next_data_with_index in next_data_with_index_list:  #对划分后的每个新数据集递归创建树\n",
    "                next_data=[x[1] for x in next_data_with_index]\n",
    "                next_data_index=[x[0] for x in next_data_with_index]\n",
    "                #print( next_data)\n",
    "                feature_value=next_data[0][best_feature_index]  #最优特征在当前数据集中的取值\n",
    "                if len(feature_list)>1:\n",
    "                    new_node.tree[feature_value]=dfs(\n",
    "                        [x[:best_feature_index]+x[best_feature_index+1:] for x in next_data],\n",
    "                        feature_list[:best_feature_index]+feature_list[best_feature_index+1:],\n",
    "                        depth+1,next_data_index)\n",
    "                else:new_node.tree[feature_value]=DecisionTreeNode(label=ID3.most_class(next_data))\n",
    "\n",
    "            return new_node\n",
    "        data_index=list(range(len(data)))\n",
    "        self.root=dfs(data,list(range(len(data[0])-1)),1,data_index)\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data):#选取最优的特征\n",
    "        \n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        information_gain_list=[]  #每个特征对数据集的信息增益\n",
    "        for i in range(n_features):\n",
    "            information_gain_list.append(ent-ID3.conditional_entropy(data,i))\n",
    "        \n",
    "        #获取最大的信息增益对应的特征索引及信息增益值\n",
    "        min_index, min_number = max(enumerate(information_gain_list), key=operator.itemgetter(1))  \n",
    "        return min_index, min_number\n",
    "    \n",
    "class C45(ID3):\n",
    "    '''\n",
    "    C4.5算法\n",
    "    '''\n",
    "    def chooseBestFeature(self,data):#选取最优的特征\n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        information_gain_ratio_list=[]  #每个特征对数据集的信息增益比\n",
    "        for i in range(n_features):\n",
    "            \n",
    "            split_data=ID3.data_divide(data,i)  #按当前特征划分数据集\n",
    "            h=-sum([len(x)/n * math.log(len(x)/n,2) for x in split_data])  #数据集关于当前特征的值的熵\n",
    "            \n",
    "            information_gain_ratio_list.append((ent-ID3.conditional_entropy(data,i))/h)\n",
    "        \n",
    "        #获取最大的信息增益比对应的特征索引及信息增益比的值\n",
    "        min_index, min_number = max(enumerate(information_gain_ratio_list), key=operator.itemgetter(1))  \n",
    "        return min_index, min_number\n",
    "\n",
    "class CARTClassifier(ID3):\n",
    "    '''\n",
    "    CART分类算法\n",
    "    '''\n",
    "    def fit(self,data,sample_weight=None):\n",
    "        def dfs(new_data,depth,data_index):  #递归创建树\n",
    "            #当前节点样本个数小于阈值 or 数据集的Gini指数小于阈值 or 树深度大于max_depth时，停止\n",
    "            if len(new_data)<self.min_samples_leaf or ID3.gini(new_data)<self.epsilon or depth>=self.max_depth:  \n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                new_node.data_index=data_index\n",
    "                return new_node\n",
    "            \n",
    "            best_feature,best_value=self.chooseBestFeature(new_data,sample_weight)  #选取最优的特征\n",
    "\n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.label=ID3.most_class(new_data)\n",
    "            new_node.data_index=data_index\n",
    "\n",
    "            #用最优特征及特征值划分当前数据集\n",
    "            next_data_with_index_list=ID3.data_divide(new_data,best_feature,data_index=data_index,value=best_value)\n",
    "            \n",
    "            next_data_index0,next_data0=[x[0] for x in next_data_with_index_list[0]],[x[1] for x in next_data_with_index_list[0]]\n",
    "            next_data_index1,next_data1=[x[0] for x in next_data_with_index_list[1]],[x[1] for x in next_data_with_index_list[1]]\n",
    "            \n",
    "            if type(best_value)==str:symbol_left,symbol_right=\"=\"+best_value,\"≠\"+best_value\n",
    "            else:symbol_left,symbol_right=\"<\"+str(best_value),\">=\"+str(best_value)\n",
    "            \n",
    "            new_node.tree[symbol_left]=dfs(next_data0,depth+1,next_data_index0)\n",
    "            new_node.tree[symbol_right]=dfs(next_data1,depth+1,next_data_index1)\n",
    "            return new_node\n",
    "        \n",
    "        data_index=list(range(len(data)))\n",
    "        self.root=dfs(data,1,data_index)\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data,sample_weight=None):#选取最优的特征及特征值\n",
    "        \n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        \n",
    "        if sample_weight:\n",
    "            min_error,best_feature,best_value=float(\"Inf\"),None,None\n",
    "            data,sample_weight=np.array(data),np.array(sample_weight).reshape((-1,1))\n",
    "            data=np.hstack((data,sample_weight)).tolist()   #将样本权重添至数据最后一列，以便划分数据时计算误差\n",
    "            \n",
    "            for i in range(n_features):\n",
    "                values=list(set([x[i] for x in data]))\n",
    "                for value in values:\n",
    "                    \n",
    "                    curr_error=self.cal_error(data,i,value)\n",
    "                    if curr_error<min_error:\n",
    "                        min_error=curr_error\n",
    "                        best_feature,best_value=i,value\n",
    "        else:            \n",
    "            min_gini,best_feature,best_value=float(\"Inf\"),None,None  #每个特征对数据集的信息增益比\n",
    "            for i in range(n_features):\n",
    "                values=list(set([x[i] for x in data]))\n",
    "                for value in values:\n",
    "                    curr_gini=ID3.gini(data,i,value)\n",
    "                    if curr_gini<min_gini:\n",
    "                        min_gini=curr_gini\n",
    "                        best_feature,best_value=i,value\n",
    "            \n",
    "        return best_feature,best_value\n",
    "    \n",
    "    def cal_error(self,data,a,value):\n",
    "        #计算分类误差\n",
    "        error=0\n",
    "        new_data=ID3.data_divide(data,a,value=value)\n",
    "        if new_data[0]!=[]:class0=Counter([x[-2] for x in new_data[0]]).most_common(1)[0][0]   #统计出现次数最多的类\n",
    "        if new_data[1]!=[]:class1=Counter([x[-2] for x in new_data[1]]).most_common(1)[0][0]\n",
    "        \n",
    "        for x in new_data[0]:\n",
    "            if x[-2]!=class0:error+=x[-1]\n",
    "        for x in new_data[1]:\n",
    "            if x[-2]!=class1:error+=x[-1]\n",
    "        \n",
    "        return error\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                if type(curr_data[curr_node.feature])==str:\n",
    "                    if \"=\"+curr_data[curr_node.feature] in curr_node.tree:curr_node=list(curr_node.tree.values())[0]\n",
    "                    else:curr_node=list(curr_node.tree.values())[1]\n",
    "                else:\n",
    "                    if curr_data[curr_node.feature]<float(list(curr_node.tree.keys())[0][1:]):curr_node=list(curr_node.tree.values())[0]\n",
    "                    else:curr_node=list(curr_node.tree.values())[1]\n",
    "                    \n",
    "                \n",
    "            pre.append(curr_node.label)\n",
    "\n",
    "        return pre\n",
    "    \n",
    "class CARTRegressor(BaseDecisionTree):\n",
    "    '''\n",
    "    CART回归算法\n",
    "    '''        \n",
    "    def fit(self,data,sample_weight=None):\n",
    "        #sample_weight用于adaboost\n",
    "        def dfs(new_data,depth,data_index):  #递归创建树\n",
    "            #当前节点样本个数小于阈值 or 数据集的MSE小于阈值 or 树深度大于max_depth时，停止\n",
    "            if len(new_data)<self.min_samples_leaf or self.cal_mse(new_data,sample_weight)[1]<self.epsilon or depth>=self.max_depth:  \n",
    "                new_node=DecisionTreeNode()\n",
    "                if not self.is_gradient:new_node.label=self.cal_mse(new_data,sample_weight)[0]\n",
    "                else:new_node.label=self.cal_gamma([x[-1] for x in new_data],self.K)  #如果用于GBDT的话，将y值取出计算最佳负梯度拟合值\n",
    "                new_node.data_index=data_index\n",
    "                return new_node\n",
    "            \n",
    "            #选取最优的特征\n",
    "            best_feature,best_split_point,min_mse,next_data_with_index_list=self.chooseBestFeature(new_data,data_index,sample_weight)  \n",
    "\n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.split_point=best_split_point\n",
    "            if not self.is_gradient:new_node.label=self.cal_mse(new_data)[0]\n",
    "            else:new_node.label=self.cal_gamma([x[-1] for x in new_data],self.K) \n",
    "            new_node.data_index=data_index\n",
    "\n",
    "            next_data_index0,next_data0=[x[0] for x in next_data_with_index_list[0]],[x[1] for x in next_data_with_index_list[0]]\n",
    "            next_data_index1,next_data1=[x[0] for x in next_data_with_index_list[1]],[x[1] for x in next_data_with_index_list[1]]\n",
    "            new_node.tree[\"<=\"+str(best_split_point)]=dfs(next_data0,depth+1,next_data_index0)\n",
    "            new_node.tree[\">\"+str(best_split_point)]=dfs(next_data1,depth+1,next_data_index1)\n",
    "            return new_node\n",
    "        \n",
    "        data_index=list(range(len(data)))\n",
    "        self.root=dfs(data,1,data_index)\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data,data_index,sample_weight):#选取最优的特征及特征值\n",
    "        \n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        min_mse,best_feature,best_split_point,best_new_data_with_index_list=float(\"Inf\"),None,None,None  #每个特征对数据集的信息增益比\n",
    "        for i in range(n_features):   #遍历特征\n",
    "            values=sorted(set([x[i] for x in data]))\n",
    "            split_points=[(values[i]+values[i+1])/2 for i in range(len(values)-1)]\n",
    "            for split_point in split_points:  #对特征i扫描切分点\n",
    "                new_data_with_index_list=self.data_split(data,i,split_point,data_index)\n",
    "                new_data_index0,new_data0=[x[0] for x in new_data_with_index_list[0]],[x[1] for x in new_data_with_index_list[0]]\n",
    "                new_data_index1,new_data1=[x[0] for x in new_data_with_index_list[1]],[x[1] for x in new_data_with_index_list[1]]\n",
    "                \n",
    "                c1,mse1=self.cal_mse(new_data0,sample_weight)\n",
    "                c2,mse2=self.cal_mse(new_data1,sample_weight)\n",
    "                \n",
    "                mse=mse1+mse2\n",
    "                if mse<min_mse:\n",
    "                    min_mse,best_feature,best_split_point,best_new_data_with_index_list=mse,i,split_point,new_data_with_index_list\n",
    "                \n",
    "        return best_feature,best_split_point,min_mse,best_new_data_with_index_list\n",
    "    \n",
    "    def data_split(self,data,a,split_point,data_index):\n",
    "        new_data=[[],[]]\n",
    "        i=0\n",
    "        for curr_data in data:\n",
    "            if curr_data[a]<=split_point:new_data[0].append((data_index[i],curr_data))\n",
    "            else:new_data[1].append((data_index[i],curr_data))\n",
    "            i+=1\n",
    "        return new_data\n",
    "    \n",
    "    def cal_mse(self,data,sample_weight=None):\n",
    "        c=sum([x[-1] for x in data])/len(data)\n",
    "        if sample_weight==None:mse=sum([(x[-1]-c)**2 for x in data])\n",
    "        else :mse=sum([y*(x[-1]-c)**2 for (x,y) in zip(data,sample_weight)])\n",
    "        return c,mse\n",
    "    \n",
    "    def cal_gamma(self,data,K):\n",
    "        #用于GBDT时，求叶子节点的最佳负梯度拟合值\n",
    "        temp1=sum(data)\n",
    "        temp2=sum([abs(x)*(1-abs(x)) for x in data])\n",
    "        gamma=(K-1)*temp1/(K*temp2)\n",
    "        return gamma\n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                if curr_data[curr_node.feature]<=curr_node.split_point:curr_node=list(curr_node.tree.values())[0]\n",
    "                else:curr_node=list(curr_node.tree.values())[1]\n",
    "            pre.append(curr_node.label)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================C45分类结果================================\n",
      "树可视化： {'label': '是是', 'feature': '有自己的房子', 'tree': {'否': {'label': '否否', 'feature': '有工作', 'tree': {'否': {'label': '否否', 'feature': None, 'tree': {}}, '是': {'label': '是是', 'feature': None, 'tree': {}}}}, '是': {'label': '是是', 'feature': None, 'tree': {}}}}\n",
      "预测结果： ['否否']\n",
      "叶子节点信息： [('否否', [0, 1, 4, 5, 6, 14]), ('是是', [2, 12, 13]), ('是是', [3, 7, 8, 9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "datasets = [['青年', '否', '否', '一般', '否否'],\n",
    "           ['青年', '否', '否', '好', '否否'],\n",
    "           ['青年', '是', '否', '好', '是是'],\n",
    "           ['青年', '是', '是', '一般', '是是'],\n",
    "           ['青年', '否', '否', '一般', '否否'],\n",
    "           ['中年', '否', '否', '一般', '否否'],\n",
    "           ['中年', '否', '否', '好', '否否'],\n",
    "           ['中年', '是', '是', '好', '是是'],\n",
    "           ['中年', '否', '是', '非常好', '是是'],\n",
    "           ['中年', '否', '是', '非常好', '是是'],\n",
    "           ['老年', '否', '是', '非常好', '是是'],\n",
    "           ['老年', '否', '是', '好', '是是'],\n",
    "           ['老年', '是', '否', '好', '是是'],\n",
    "           ['老年', '是', '否', '非常好', '是是'],\n",
    "           ['老年', '否', '否', '一般', '否否'],\n",
    "           ]\n",
    "labels = [u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别']\n",
    "print('================================C45分类结果================================')\n",
    "tree=C45(min_samples_leaf=1,max_depth=5)\n",
    "tree.fit(datasets)\n",
    "\n",
    "print('树可视化：',tree.root.display(labels))\n",
    "\n",
    "print('预测结果：',tree.predict([['老年', '否', '否', '一般']]))\n",
    "print('叶子节点信息：',tree.root.print_leaf_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ID3分类结果================================\n",
      "树可视化： {'label': '是是', 'feature': '有自己的房子', 'tree': {'否': {'label': '否否', 'feature': '有工作', 'tree': {'否': {'label': '否否', 'feature': None, 'tree': {}}, '是': {'label': '是是', 'feature': None, 'tree': {}}}}, '是': {'label': '是是', 'feature': None, 'tree': {}}}}\n",
      "预测结果： ['否否']\n",
      "叶子节点信息： [('否否', [0, 1, 4, 5, 6, 14]), ('是是', [2, 12, 13]), ('是是', [3, 7, 8, 9, 10, 11])]\n"
     ]
    }
   ],
   "source": [
    "print('================================ID3分类结果================================')\n",
    "tree=ID3()\n",
    "tree.fit(datasets)\n",
    "\n",
    "print('树可视化：',tree.root.display(labels))\n",
    "\n",
    "print('预测结果：',tree.predict([['老年', '否', '否', '一般']]))\n",
    "print('叶子节点信息：',tree.root.print_leaf_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================CART分类结果================================\n",
      "树可视化： {'label': '是是', 'feature': '有自己的房子', 'tree': {'=是': {'label': '是是', 'feature': None, 'tree': {}}, '≠是': {'label': '否否', 'feature': '有工作', 'tree': {'=是': {'label': '是是', 'feature': None, 'tree': {}}, '≠是': {'label': '否否', 'feature': None, 'tree': {}}}}}}\n",
      "预测结果： ['否否']\n",
      "叶子节点信息： [('是是', [3, 7, 8, 9, 10, 11]), ('是是', [2, 12, 13]), ('否否', [0, 1, 4, 5, 6, 14])]\n"
     ]
    }
   ],
   "source": [
    "print('================================CART分类结果================================')\n",
    "tree=CARTClassifier()\n",
    "tree.fit(datasets)\n",
    "\n",
    "print('树可视化：',tree.root.display(labels))\n",
    "\n",
    "print('预测结果：',tree.predict([['老年', '否', '否', '一般']]))\n",
    "print('叶子节点信息：',tree.root.print_leaf_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================CART回归结果================================\n",
      "{'label': 6.618, 'feature': '维度一', 'tree': {'<=5.5': {'label': 5.0600000000000005, 'feature': '维度一', 'tree': {'<=3.5': {'label': 4.72, 'feature': '维度一', 'tree': {'<=1.5': {'label': 4.5, 'feature': None, 'tree': {}}, '>1.5': {'label': 4.83, 'feature': '维度一', 'tree': {'<=2.5': {'label': 4.75, 'feature': None, 'tree': {}}, '>2.5': {'label': 4.91, 'feature': None, 'tree': {}}}, 'split_point': 2.5}}, 'split_point': 1.5}, '>3.5': {'label': 5.57, 'feature': '维度一', 'tree': {'<=4.5': {'label': 5.34, 'feature': None, 'tree': {}}, '>4.5': {'label': 5.8, 'feature': None, 'tree': {}}}, 'split_point': 4.5}}, 'split_point': 3.5}, '>5.5': {'label': 8.175999999999998, 'feature': '维度一', 'tree': {'<=7.5': {'label': 7.475, 'feature': '维度一', 'tree': {'<=6.5': {'label': 7.05, 'feature': None, 'tree': {}}, '>6.5': {'label': 7.9, 'feature': None, 'tree': {}}}, 'split_point': 6.5}, '>7.5': {'label': 8.643333333333333, 'feature': '维度一', 'tree': {'<=8.5': {'label': 8.23, 'feature': None, 'tree': {}}, '>8.5': {'label': 8.85, 'feature': '维度一', 'tree': {'<=9.5': {'label': 8.7, 'feature': None, 'tree': {}}, '>9.5': {'label': 9.0, 'feature': None, 'tree': {}}}, 'split_point': 9.5}}, 'split_point': 8.5}}, 'split_point': 7.5}}, 'split_point': 5.5}\n",
      "预测结果： [4.75]\n",
      "叶子节点信息： [(4.5, [0]), (4.75, [1]), (4.91, [2]), (5.34, [3]), (5.8, [4]), (7.05, [5]), (7.9, [6]), (8.23, [7]), (8.7, [8]), (9.0, [9])]\n"
     ]
    }
   ],
   "source": [
    "datasets = [[1,4.5],\n",
    "           [2,4.75],\n",
    "           [3,4.91],\n",
    "           [4,5.34],\n",
    "           [5,5.80],\n",
    "           [6,7.05],\n",
    "           [7,7.9],\n",
    "           [8,8.23],\n",
    "           [9,8.7],\n",
    "           [10,9.0]]\n",
    "print('================================CART回归结果================================')\n",
    "labels = ['维度一']\n",
    "tree=CARTRegressor(max_depth=5)\n",
    "tree.fit(datasets)\n",
    "\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "print('预测结果：',tree.predict([[1.8]]))\n",
    "print('叶子节点信息：',tree.root.print_leaf_node())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================CART训练iris数据集========================\n",
      "预测结果： [0.0, 1.0, 2.0, 1.0, 0.0, 1.0, 1.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 0.0, 2.0, 2.0, 2.0, 0.0, 1.0, 2.0]\n",
      "========================sklearn实现========================\n",
      "预测结果 [0 1 2 1 0 1 1 0 2 0 0 2 2 2 0 2 2 2 1 1 1 1 2 0 2 2 2 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "print(\"========================CART训练iris数据集========================\")\n",
    "model=CARTClassifier()\n",
    "model.fit(np.hstack((X_train,y_train.reshape((-1,1)))))\n",
    "print('预测结果：',model.predict(X_test))\n",
    "\n",
    "print(\"========================sklearn实现========================\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train,y_train)\n",
    "print('预测结果',clf.predict(X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
