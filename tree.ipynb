{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0==None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from abc import abstractmethod\n",
    "import operator\n",
    "\n",
    "# 定义节点类\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self,  label=None, feature_name=None, feature=None,value=None,split_point=None):\n",
    "        self.label = label\n",
    "        self.feature_name = feature_name\n",
    "        self.feature = feature\n",
    "        self.tree = {}\n",
    "        self.value=value\n",
    "        self.split_point=split_point\n",
    "    \n",
    "    def feature_name(self):\n",
    "        return self.feature_name_list(self.feature)\n",
    "    \n",
    "    def display(self,feature_name_list=None):\n",
    "        '''\n",
    "        将树打印出来'''\n",
    "        if feature_name_list:featurename=feature_name_list[self.feature] if self.feature!=None else None\n",
    "        else:featurename=self.feature\n",
    "        res={'label':self.label,'feature':featurename,'tree':{}}\n",
    "\n",
    "        if self.value:res['value']=self.value\n",
    "        if self.split_point:res['split_point']=self.split_point\n",
    "        for next_node in self.tree:\n",
    "            res['tree'][next_node]=self.tree[next_node].display(feature_name_list)\n",
    "        return res\n",
    "\n",
    "class BaseDecisionTree:\n",
    "    def __init__(self,epsilon=1e-3,min_samples_leaf=1):\n",
    "        self.root=None\n",
    "        self.epsilon=epsilon  # 信息增益/信息增益比/Gini小于该阈值时，算法停止\n",
    "        self.min_samples_leaf=min_samples_leaf  #叶子节点拥有的样本最小个数，当节点样本个数小于该阈值时算法停止\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def __init__(self,\n",
    "                 criterion,\n",
    "                 splitter,\n",
    "                 max_depth,\n",
    "                 min_samples_split,\n",
    "                 min_samples_leaf,\n",
    "                 min_weight_fraction_leaf,\n",
    "                 max_features,\n",
    "                 max_leaf_nodes,\n",
    "                 random_state,\n",
    "                 min_impurity_decrease,\n",
    "                 min_impurity_split,\n",
    "                 class_weight=None,\n",
    "                 presort=False):\n",
    "        self.criterion = criterion\n",
    "        self.splitter = splitter\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.max_leaf_nodes = max_leaf_nodes\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "        self.min_impurity_split = min_impurity_split\n",
    "        self.class_weight = class_weight\n",
    "        self.presort = presort\n",
    "'''\n",
    "    @staticmethod\n",
    "    def entropy(data):\n",
    "        '''\n",
    "        输入数据data,输出其经验熵'''\n",
    "        n=len(data)   #数据个数\n",
    "        label_dict={}\n",
    "        for i in range(n):\n",
    "            label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "        k=len(label_dict)  #类别个数\n",
    "        ent=0\n",
    "        for n_k in label_dict.values():\n",
    "            ent+= n_k/n * math.log(n_k/n,2)\n",
    "        return -ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def conditional_entropy(data,a):\n",
    "        '''\n",
    "        输入数据data和用来分类的特征a(即数据的第a列),输出条件熵'''\n",
    "        n=len(data)   #数据个数\n",
    "        con_ent=0\n",
    "        new_data=BaseDecisionTree.data_divide(data,a)\n",
    "        for curr_data in new_data:\n",
    "            con_ent+= len(curr_data)/n * BaseDecisionTree.entropy(curr_data)        \n",
    "        return con_ent\n",
    "    \n",
    "    @staticmethod\n",
    "    def gini(data,a=None,value=None):\n",
    "        n=len(data)  \n",
    "        if not a:\n",
    "            label_dict={} \n",
    "            for i in range(n):\n",
    "                label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "            return 1-sum((x/n)**2 for x in label_dict.values())\n",
    "        else:\n",
    "            new_data=BaseDecisionTree.data_divide(data,a,value)\n",
    "            return len(new_data[0])/n*BaseDecisionTree.gini(new_data[0]) + len(new_data[1])/n*BaseDecisionTree.gini(new_data[1])\n",
    "            \n",
    "    @staticmethod\n",
    "    def data_divide(data,a,value=None):\n",
    "        '''\n",
    "        根据第a列特征将数据划分\n",
    "        如果输入特征a的某个value，将数据集按a=value和a≠value划分成两个\n",
    "        如果没有输入某个value，将数据集按a所有特征划分'''\n",
    "        if not value:\n",
    "            new_data={}\n",
    "            for curr_data in data:\n",
    "                new_data[curr_data[a]]=new_data.get(curr_data[a],[])\n",
    "                new_data[curr_data[a]].append(curr_data)\n",
    "            return list(new_data.values())\n",
    "        else:\n",
    "            new_data=[[],[]]\n",
    "            for curr_data in data:\n",
    "                if curr_data[a]==value:\n",
    "                    new_data[0].append(curr_data)\n",
    "                else:\n",
    "                    new_data[1].append(curr_data)\n",
    "            return new_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def most_class(data):\n",
    "        '''\n",
    "        返回数据集中实例数最多的类'''\n",
    "        n=len(data)   #数据个数\n",
    "        label_dict={}\n",
    "        for i in range(n):\n",
    "            label_dict[data[i][-1]]=label_dict.get(data[i][-1],0)+1\n",
    "        m=0\n",
    "        for key in label_dict.keys():\n",
    "            if label_dict[key]>m:\n",
    "                m=label_dict[key]\n",
    "                res=key\n",
    "        return res\n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                curr_node=curr_node.tree[curr_data[curr_node.feature]]\n",
    "            pre.append(curr_node.label)\n",
    "        return pre\n",
    "    \n",
    "class ID3(BaseDecisionTree):\n",
    "    '''\n",
    "    ID3算法\n",
    "    '''\n",
    "\n",
    "    def fit(self,data):\n",
    "        def dfs(new_data,feature_list):  #递归创建树\n",
    "            if len(new_data)<self.min_samples_leaf:  #当前节点样本个数小于阈值，停止\n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                return new_node\n",
    "\n",
    "            best_feature_index,information_gain=self.chooseBestFeature(new_data)  #选取最优的特征\n",
    "            best_feature=feature_list[best_feature_index]\n",
    "\n",
    "            if information_gain<self.epsilon:   #当信息增益小于阈值epsilon，停止\n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                return new_node\n",
    "            \n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.label=ID3.most_class(new_data)\n",
    "            \n",
    "            next_data_list=ID3.data_divide(new_data,best_feature)  #用最优的特征划分当前数据集\n",
    "            for next_data in next_data_list:  #对划分后的每个新数据集递归创建树\n",
    "                feature_value=next_data[0][best_feature_index]  #最优特征在当前数据集中的取值\n",
    "                if len(feature_list)>1:\n",
    "                    new_node.tree[feature_value]=dfs(\n",
    "                        [x[:best_feature_index]+x[best_feature_index+1:] for x in next_data],\n",
    "                        feature_list[:best_feature_index]+feature_list[best_feature_index+1:])\n",
    "                else:new_node.tree[feature_value]=DecisionTreeNode(label=ID3.most_class(next_data))\n",
    "\n",
    "            return new_node\n",
    "        self.root=dfs(data,list(range(len(data[0])-1)))\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data):#选取最优的特征\n",
    "        \n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        information_gain_list=[]  #每个特征对数据集的信息增益\n",
    "        for i in range(n_features):\n",
    "            information_gain_list.append(ent-ID3.conditional_entropy(data,i))\n",
    "        \n",
    "        #获取最大的信息增益对应的特征索引及信息增益值\n",
    "        min_index, min_number = max(enumerate(information_gain_list), key=operator.itemgetter(1))  \n",
    "        return min_index, min_number\n",
    "    \n",
    "class C45(ID3):\n",
    "    '''\n",
    "    C4.5算法\n",
    "    '''\n",
    "    def chooseBestFeature(self,data):#选取最优的特征\n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        information_gain_ratio_list=[]  #每个特征对数据集的信息增益比\n",
    "        for i in range(n_features):\n",
    "            \n",
    "            split_data=ID3.data_divide(data,i)  #按当前特征划分数据集\n",
    "            h=-sum([len(x)/n * math.log(len(x)/n,2) for x in split_data])  #数据集关于当前特征的值的熵\n",
    "            \n",
    "            information_gain_ratio_list.append((ent-ID3.conditional_entropy(data,i))/h)\n",
    "        \n",
    "        #获取最大的信息增益比对应的特征索引及信息增益比的值\n",
    "        min_index, min_number = max(enumerate(information_gain_ratio_list), key=operator.itemgetter(1))  \n",
    "        return min_index, min_number\n",
    "\n",
    "class CARTClassifier(ID3):\n",
    "    '''\n",
    "    CART分类算法\n",
    "    '''\n",
    "    def fit(self,data):\n",
    "        def dfs(new_data):  #递归创建树\n",
    "            #当前节点样本个数小于阈值或数据集的Gini指数小于阈值时，停止\n",
    "            if len(new_data)<self.min_samples_leaf or ID3.gini(new_data)<self.epsilon:  \n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=ID3.most_class(new_data)\n",
    "                return new_node\n",
    "            \n",
    "            best_feature,best_value,min_gini=self.chooseBestFeature(new_data)  #选取最优的特征\n",
    "\n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.label=ID3.most_class(new_data)\n",
    "\n",
    "            next_data_list=ID3.data_divide(new_data,best_feature,best_value)  #用最优的特征及特征值划分当前数据集\n",
    "            new_node.tree[\"=\"+best_value]=dfs(next_data_list[0])\n",
    "            new_node.tree[\"≠\"+best_value]=dfs(next_data_list[1])\n",
    "            return new_node\n",
    "        \n",
    "        self.root=dfs(data)\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data):#选取最优的特征及特征值\n",
    "        \n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        min_gini,best_feature,best_value=float(\"Inf\"),None,None  #每个特征对数据集的信息增益比\n",
    "        for i in range(n_features):\n",
    "            values=list(set([x[i] for x in data]))\n",
    "            for value in values:\n",
    "                curr_gini=ID3.gini(data,i,value)\n",
    "                if curr_gini<min_gini:\n",
    "                    min_gini=curr_gini\n",
    "                    best_feature,best_value=i,value\n",
    "        return best_feature,best_value,min_gini\n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                if \"=\"+curr_data[curr_node.feature] in curr_node.tree:curr_node=list(curr_node.tree.values())[0]\n",
    "                else:curr_node=list(curr_node.tree.values())[1]\n",
    "            pre.append(curr_node.label)\n",
    "        return pre\n",
    "    \n",
    "class CARTRegressor(BaseDecisionTree):\n",
    "    '''\n",
    "    CART回归算法\n",
    "    '''\n",
    "    def fit(self,data):\n",
    "        def dfs(new_data):  #递归创建树\n",
    "            #当前节点样本个数小于阈值或数据集的MSE小于阈值时，停止\n",
    "            if len(new_data)<self.min_samples_leaf or self.cal_mse(new_data)[1]<self.epsilon:  \n",
    "                new_node=DecisionTreeNode()\n",
    "                new_node.label=self.cal_mse(new_data)[0]\n",
    "                return new_node\n",
    "            \n",
    "            best_feature,best_split_point,min_mse,next_data_list=self.chooseBestFeature(new_data)  #选取最优的特征\n",
    "\n",
    "            new_node=DecisionTreeNode()\n",
    "            new_node.feature=best_feature\n",
    "            new_node.split_point=best_split_point\n",
    "            new_node.label=self.cal_mse(new_data)[0]\n",
    "\n",
    "            new_node.tree[\"<=\"+str(best_split_point)]=dfs(next_data_list[0])\n",
    "            new_node.tree[\">\"+str(best_split_point)]=dfs(next_data_list[1])\n",
    "            return new_node\n",
    "        \n",
    "        self.root=dfs(data)\n",
    "        \n",
    "        return self.root\n",
    "    \n",
    "    def chooseBestFeature(self,data):#选取最优的特征及特征值\n",
    "        \n",
    "        ent=ID3.entropy(data)  #数据集的经验熵\n",
    "        n_features=len(data[0])-1  #特征个数\n",
    "        n=len(data)  #数据个数\n",
    "        min_mse,best_feature,best_split_point,best_new_data=float(\"Inf\"),None,None,None  #每个特征对数据集的信息增益比\n",
    "        for i in range(n_features):   #遍历特征\n",
    "            values=sorted(set([x[i] for x in data]))\n",
    "            split_points=[(values[i]+values[i+1])/2 for i in range(len(values)-1)]\n",
    "            for split_point in split_points:  #对特征i扫描切分点\n",
    "                new_data=self.data_split(data,i,split_point)\n",
    "                \n",
    "                c1,mse1=self.cal_mse(new_data[0])\n",
    "                c2,mse2=self.cal_mse(new_data[1])\n",
    "                \n",
    "                mse=mse1+mse2\n",
    "                if mse<min_mse:\n",
    "                    min_mse,best_feature,best_split_point,best_new_data=mse,i,split_point,new_data\n",
    "                \n",
    "        return best_feature,best_split_point,min_mse,best_new_data\n",
    "    \n",
    "    def data_split(self,data,a,split_point):\n",
    "        new_data=[[],[]]\n",
    "        for curr_data in data:\n",
    "            if curr_data[a]<=split_point:new_data[0].append(curr_data)\n",
    "            else:new_data[1].append(curr_data)\n",
    "        return new_data\n",
    "    \n",
    "    def cal_mse(self,data):\n",
    "        c=sum([x[-1] for x in data])/len(data)\n",
    "        mse=sum([(x[-1]-c)**2 for x in data])\n",
    "        return c,mse\n",
    "    \n",
    "    def predict(self,data):\n",
    "        pre=[]\n",
    "        for curr_data in data:\n",
    "            curr_node=self.root\n",
    "            while curr_node.tree:\n",
    "                if curr_data[curr_node.feature]<=curr_node.split_point:curr_node=list(curr_node.tree.values())[0]\n",
    "                else:curr_node=list(curr_node.tree.values())[1]\n",
    "            pre.append(curr_node.label)\n",
    "        return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [['青年', '否', '否', '一般', '否否'],\n",
    "           ['青年', '否', '否', '好', '否否'],\n",
    "           ['青年', '是', '否', '好', '是是'],\n",
    "           ['青年', '是', '是', '一般', '是是'],\n",
    "           ['青年', '否', '否', '一般', '否否'],\n",
    "           ['中年', '否', '否', '一般', '否否'],\n",
    "           ['中年', '否', '否', '好', '否否'],\n",
    "           ['中年', '是', '是', '好', '是是'],\n",
    "           ['中年', '否', '是', '非常好', '是是'],\n",
    "           ['中年', '否', '是', '非常好', '是是'],\n",
    "           ['老年', '否', '是', '非常好', '是是'],\n",
    "           ['老年', '否', '是', '好', '是是'],\n",
    "           ['老年', '是', '否', '好', '是是'],\n",
    "           ['老年', '是', '否', '非常好', '是是'],\n",
    "           ['老年', '否', '否', '一般', '否否'],\n",
    "           ]\n",
    "labels = [u'年龄', u'有工作', u'有自己的房子', u'信贷情况', u'类别']\n",
    "\n",
    "tree=C45()\n",
    "tree.fit(datasets)\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "tree.predict([['老年', '否', '否', '一般']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=ID3()\n",
    "tree.fit(datasets)\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "tree.predict([['老年', '否', '否', '一般']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree=CARTClassifier()\n",
    "tree.fit(datasets)\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "tree.predict([['老年', '否', '否', '一般']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.5\n",
      "0 3.5\n",
      "0 1.5\n",
      "0 2.5\n",
      "0 4.5\n",
      "0 7.5\n",
      "0 6.5\n",
      "0 8.5\n",
      "0 9.5\n",
      "{'label': 6.618, 'feature': '维度一', 'tree': {'<=5.5': {'label': 5.0600000000000005, 'feature': '维度一', 'tree': {'<=3.5': {'label': 4.72, 'feature': '维度一', 'tree': {'<=1.5': {'label': 4.5, 'feature': None, 'tree': {}}, '>1.5': {'label': 4.83, 'feature': '维度一', 'tree': {'<=2.5': {'label': 4.75, 'feature': None, 'tree': {}}, '>2.5': {'label': 4.91, 'feature': None, 'tree': {}}}, 'split_point': 2.5}}, 'split_point': 1.5}, '>3.5': {'label': 5.57, 'feature': '维度一', 'tree': {'<=4.5': {'label': 5.34, 'feature': None, 'tree': {}}, '>4.5': {'label': 5.8, 'feature': None, 'tree': {}}}, 'split_point': 4.5}}, 'split_point': 3.5}, '>5.5': {'label': 8.175999999999998, 'feature': '维度一', 'tree': {'<=7.5': {'label': 7.475, 'feature': '维度一', 'tree': {'<=6.5': {'label': 7.05, 'feature': None, 'tree': {}}, '>6.5': {'label': 7.9, 'feature': None, 'tree': {}}}, 'split_point': 6.5}, '>7.5': {'label': 8.643333333333333, 'feature': '维度一', 'tree': {'<=8.5': {'label': 8.23, 'feature': None, 'tree': {}}, '>8.5': {'label': 8.85, 'feature': '维度一', 'tree': {'<=9.5': {'label': 8.7, 'feature': None, 'tree': {}}, '>9.5': {'label': 9.0, 'feature': None, 'tree': {}}}, 'split_point': 9.5}}, 'split_point': 8.5}}, 'split_point': 7.5}}, 'split_point': 5.5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.75]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets = [[1,4.5],\n",
    "           [2,4.75],\n",
    "           [3,4.91],\n",
    "           [4,5.34],\n",
    "           [5,5.80],\n",
    "           [6,7.05],\n",
    "           [7,7.9],\n",
    "           [8,8.23],\n",
    "           [9,8.7],\n",
    "           [10,9.0]]\n",
    "labels = ['维度一']\n",
    "tree=CARTRegressor(min_samples_leaf=2)\n",
    "tree.fit(datasets)\n",
    "\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "tree.predict([[1.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.5\n",
      "0 3.5\n",
      "0 7.5\n",
      "{'label': 6.618, 'feature': '维度一', 'tree': {'<=5.5': {'label': 5.0600000000000005, 'feature': '维度一', 'tree': {'<=3.5': {'label': 4.72, 'feature': None, 'tree': {}}, '>3.5': {'label': 5.57, 'feature': None, 'tree': {}}}, 'split_point': 3.5}, '>5.5': {'label': 8.175999999999998, 'feature': '维度一', 'tree': {'<=7.5': {'label': 7.475, 'feature': None, 'tree': {}}, '>7.5': {'label': 8.643333333333333, 'feature': None, 'tree': {}}}, 'split_point': 7.5}}, 'split_point': 5.5}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.72]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree=CARTRegressor(min_samples_leaf=5)\n",
    "tree.fit(datasets)\n",
    "\n",
    "print(tree.root.display(labels))\n",
    "\n",
    "tree.predict([[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
